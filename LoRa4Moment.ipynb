{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1EBZN1sDVm7b5sS6WTTq0mSvYe7C2cwx0",
     "timestamp": 1770828157360
    }
   ],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "VfZNqqZi-f0C",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770826460129,
     "user_tz": 300,
     "elapsed": 33079,
     "user": {
      "displayName": "Ruimeng Liu",
      "userId": "12607272939322964553"
     }
    },
    "outputId": "0779ae0d-178e-4181-e379-026412c97af2",
    "ExecuteTime": {
     "end_time": "2026-02-12T04:15:41.367651Z",
     "start_time": "2026-02-12T04:15:09.909725Z"
    }
   },
   "source": [
    "# 1. å…‹éš† LLaMA-Factory ä»“åº“\n",
    "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "%cd LLaMA-Factory\n",
    "\n",
    "# 2. å®‰è£…ä¾èµ– (è¿™ä¸€æ­¥å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)\n",
    "!pip install -e \".[torch,metrics]\"\n",
    "!pip install bitsandbytes\n",
    "\n",
    "# 3. ç™»å½• HuggingFace (å¯é€‰ï¼Œä¸ºäº†ä¸‹è½½æŸäº›å—é™æ¨¡å‹ï¼Œä¸€èˆ¬ Qwen ä¸éœ€è¦)\n",
    "# !huggingface-cli login"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaMA-Factory'...\n",
      "remote: Enumerating objects: 614, done.\u001B[K\n",
      "remote: Counting objects: 100% (614/614), done.\u001B[K\n",
      "remote: Compressing objects: 100% (455/455), done.\u001B[K\n",
      "remote: Total 614 (delta 150), reused 378 (delta 101), pack-reused 0 (from 0)\u001B[K\n",
      "Receiving objects: 100% (614/614), 5.24 MiB | 17.76 MiB/s, done.\n",
      "Resolving deltas: 100% (150/150), done.\n",
      "/content/LLaMA-Factory\n",
      "Obtaining file:///content/LLaMA-Factory\n",
      "  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build editable ... \u001B[?25l\u001B[?25hdone\n",
      "  Installing backend dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting accelerate<=1.11.0,>=1.3.0 (from llamafactory==0.9.5.dev0)\n",
      "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting av<=16.0.0,>=10.0.0 (from llamafactory==0.9.5.dev0)\n",
      "  Downloading av-16.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: datasets<=4.0.0,>=2.16.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (4.0.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.8.2)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.128.2)\n",
      "Collecting fire (from llamafactory==0.9.5.dev0)\n",
      "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: gradio<=5.50.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (5.50.0)\n",
      "Collecting hf-transfer (from llamafactory==0.9.5.dev0)\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (3.10.0)\n",
      "Collecting modelscope (from llamafactory==0.9.5.dev0)\n",
      "  Downloading modelscope-1.34.0-py3-none-any.whl.metadata (43 kB)\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m43.3/43.3 kB\u001B[0m \u001B[31m3.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (2.0.2)\n",
      "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (2.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (26.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (2.2.2)\n",
      "Requirement already satisfied: peft<=0.18.1,>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.18.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (5.29.6)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (2.12.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (6.0.3)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.7.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (1.16.3)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.2.1)\n",
      "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (3.2.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.12.0)\n",
      "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (2.9.0+cu128)\n",
      "Requirement already satisfied: torchaudio>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (2.9.0+cu128)\n",
      "Requirement already satisfied: torchdata<=0.11.0,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.11.0)\n",
      "Requirement already satisfied: torchvision>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.24.0+cu128)\n",
      "Requirement already satisfied: transformers!=4.52.0,!=4.57.0,<=5.0.0,>=4.51.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (5.0.0)\n",
      "Collecting trl<=0.24.0,>=0.18.0 (from llamafactory==0.9.5.dev0)\n",
      "  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting tyro<0.9.0 (from llamafactory==0.9.5.dev0)\n",
      "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.5.dev0) (0.40.0)\n",
      "\u001B[33mWARNING: llamafactory 0.9.5.dev0 does not provide the extra 'metrics'\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: llamafactory 0.9.5.dev0 does not provide the extra 'torch'\u001B[0m\u001B[33m\n",
      "\u001B[0mRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.5.dev0) (5.9.5)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.5.dev0) (1.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (4.67.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (2025.3.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (4.12.1)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (1.2.0)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (1.0.0)\n",
      "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (1.14.0)\n",
      "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.28.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (3.0.3)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (3.11.7)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (11.3.0)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.0.22)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.15.0)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.50.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (4.15.0)\n",
      "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (15.0.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from fastapi->llamafactory==0.9.5.dev0) (0.4.2)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi->llamafactory==0.9.5.dev0) (0.0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.5.dev0) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.5.dev0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.5.dev0) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.5.dev0) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.5.dev0) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.5.dev0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->llamafactory==0.9.5.dev0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->llamafactory==0.9.5.dev0) (2025.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->llamafactory==0.9.5.dev0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic->llamafactory==0.9.5.dev0) (2.41.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (3.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->llamafactory==0.9.5.dev0) (3.5.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.12/dist-packages (from torchdata<=0.11.0,>=0.10.0->llamafactory==0.9.5.dev0) (2.5.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.57.0,<=5.0.0,>=4.51.0->llamafactory==0.9.5.dev0) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.57.0,<=5.0.0,>=4.51.0->llamafactory==0.9.5.dev0) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.57.0,<=5.0.0,>=4.51.0->llamafactory==0.9.5.dev0) (0.21.1)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.12/dist-packages (from tyro<0.9.0->llamafactory==0.9.5.dev0) (0.17.0)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro<0.9.0->llamafactory==0.9.5.dev0) (13.9.4)\n",
      "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.5.dev0)\n",
      "  Downloading shtab-1.8.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn->llamafactory==0.9.5.dev0) (8.3.1)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn->llamafactory==0.9.5.dev0) (0.16.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from fire->llamafactory==0.9.5.dev0) (3.3.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->llamafactory==0.9.5.dev0) (4.9.3)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (3.11)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (3.13.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio<=5.50.0,>=4.38.0->llamafactory==0.9.5.dev0) (1.0.9)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.5.dev0) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.5.dev0) (1.5.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.5.dev0) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (3.4.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.5.dev0) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.5.dev0) (2.19.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->llamafactory==0.9.5.dev0) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.5.dev0) (1.22.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.5.dev0) (0.1.2)\n",
      "Downloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m375.8/375.8 kB\u001B[0m \u001B[31m22.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading av-16.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (40.5 MB)\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m40.5/40.5 MB\u001B[0m \u001B[31m26.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hDownloading trl-0.24.0-py3-none-any.whl (423 kB)\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m423.1/423.1 kB\u001B[0m \u001B[31m43.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m109.8/109.8 kB\u001B[0m \u001B[31m12.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m115.9/115.9 kB\u001B[0m \u001B[31m13.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m3.6/3.6 MB\u001B[0m \u001B[31m131.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading modelscope-1.34.0-py3-none-any.whl (6.1 MB)\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m6.1/6.1 MB\u001B[0m \u001B[31m146.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hDownloading shtab-1.8.0-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: llamafactory\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for llamafactory: filename=llamafactory-0.9.5.dev0-py3-none-any.whl size=26985 sha256=c0f4f5ccfa86710b063aed821d9a303dac59ec1d3efe866c72e957db0e39a8d0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-gptzayly/wheels/68/8b/5e/52f9888e6a91a2651260d603137c052b925af896da6e32a3f7\n",
      "Successfully built llamafactory\n",
      "Installing collected packages: shtab, hf-transfer, fire, av, modelscope, tyro, accelerate, trl, llamafactory\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.12.0\n",
      "    Uninstalling accelerate-1.12.0:\n",
      "      Successfully uninstalled accelerate-1.12.0\n",
      "Successfully installed accelerate-1.11.0 av-16.0.0 fire-0.7.1 hf-transfer-0.1.9 llamafactory-0.9.5.dev0 modelscope-1.34.0 shtab-1.8.0 trl-0.24.0 tyro-0.8.14\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu128)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (26.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m59.1/59.1 MB\u001B[0m \u001B[31m9.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0mm\n",
      "\u001B[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.49.1\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# è¯»å–åŸæœ¬çš„ dataset_info.json\n",
    "with open('data/dataset_info.json', 'r', encoding='utf-8') as f:\n",
    "    dataset_info = json.load(f)\n",
    "\n",
    "# æ·»åŠ  Raymond çš„é…ç½®\n",
    "dataset_info['raymond_data'] = {\n",
    "    \"file_name\": \"raymond.json\",  # åˆšæ‰ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„\n",
    "    \"formatting\": \"sharegpt\",        # ä½ çš„æ•°æ®æ ¼å¼å±äº sharegpt ç±»å‹\n",
    "    \"columns\": {\n",
    "        \"messages\": \"messages\"       # å‘Šè¯‰æ¡†æ¶å»è¯» messages å­—æ®µ\n",
    "    }\n",
    "}\n",
    "\n",
    "# ä¿å­˜å›å»\n",
    "with open('data/dataset_info.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(dataset_info, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Raymond æ•°æ®é›†æ³¨å†ŒæˆåŠŸï¼\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aUdb5WDz_H8W",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770828111518,
     "user_tz": 300,
     "elapsed": 47,
     "user": {
      "displayName": "Ruimeng Liu",
      "userId": "12607272939322964553"
     }
    },
    "outputId": "60f415d5-8540-4e36-eb88-ffdd372187de"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Raymond æ•°æ®é›†æ³¨å†ŒæˆåŠŸï¼\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•\n",
    "%cd /content/LLaMA-Factory\n",
    "\n",
    "# 2. å¯åŠ¨ Web UI (ä½¿ç”¨æ–°ç‰ˆå‘½ä»¤)\n",
    "!export GRADIO_SHARE=1 && llamafactory-cli webui"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "PH_RiO2e_PaU",
    "outputId": "f7693d1d-c5c8-455f-a843-47c41dca8519"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/LLaMA-Factory\n",
      "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:44: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  re_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%\\-]+)\", re.U)\n",
      "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:46: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  re_skip_default = re.compile(\"(\\r\\n|\\s)\", re.U)\n",
      "/usr/local/lib/python3.12/dist-packages/jieba/finalseg/__init__.py:78: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  re_skip = re.compile(\"([a-zA-Z0-9]+(?:\\.\\d+)?%?)\")\n",
      "Visit http://ip:port for Web UI, e.g., http://127.0.0.1:7860\n",
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* Running on public URL: https://122536b96cfbf07c46.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. è‡ªåŠ¨å¯»æ‰¾æœ€æ–°çš„æ¨¡å‹æƒé‡è·¯å¾„\n",
    "save_root = \"/content/LLaMA-Factory/saves\"\n",
    "# æ³¨æ„ï¼šè¿™é‡Œè¯·ç¡®è®¤ä½ çš„åº•åº§æ¨¡å‹è·¯å¾„ï¼Œé€šå¸¸æ˜¯ Qwen/Qwen2.5-7B-Instruct\n",
    "base_model_path = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "adapter_path = None\n",
    "\n",
    "# éå†ç›®å½•å¯»æ‰¾ checkpoint\n",
    "if os.path.exists(save_root):\n",
    "    checkpoints = []\n",
    "    for root, dirs, files in os.walk(save_root):\n",
    "        for d in dirs:\n",
    "            if d.startswith(\"checkpoint-\"):\n",
    "                full_path = os.path.join(root, d)\n",
    "                try:\n",
    "                    step = int(d.split(\"-\")[-1])\n",
    "                    checkpoints.append((step, full_path))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "    if checkpoints:\n",
    "        # æ‰¾åˆ°æ­¥æ•°æœ€å¤§çš„é‚£ä¸ª\n",
    "        checkpoints.sort(key=lambda x: x[0], reverse=True)\n",
    "        adapter_path = checkpoints[0][1]\n",
    "        print(f\"âœ… æ‰¾åˆ°æœ€æ–°æ¨¡å‹æƒé‡: {adapter_path}\")\n",
    "    else:\n",
    "        print(\"âŒ æœªæ‰¾åˆ° checkpointï¼Œè¯·ç¡®è®¤è®­ç»ƒæ˜¯å¦è‡³å°‘è·‘å®Œäº†ä¸€æ¬¡ä¿å­˜æ­¥éª¤ã€‚\")\n",
    "else:\n",
    "    print(\"âŒ saves ç›®å½•ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿›è¡Œè®­ç»ƒã€‚\")\n",
    "\n",
    "# 2. å¦‚æœæ‰¾åˆ°äº†ï¼Œå¯åŠ¨ç‹¬ç«‹èŠå¤©ç•Œé¢\n",
    "if adapter_path:\n",
    "    print(\"ğŸš€ æ­£åœ¨å¯åŠ¨ Raymond ä¸“å±èŠå¤©å®¤...\")\n",
    "    print(\"âš ï¸ æ³¨æ„ï¼šè¿›å…¥ç½‘é¡µåï¼Œè¯·æ‰‹åŠ¨åœ¨å·¦ä¾§ 'System Prompt' æ¡†ä¸­è¾“å…¥ï¼š\")\n",
    "    print(\"   You are Raymond, a humorous chatter with a unique sarcastic style.\")\n",
    "\n",
    "    # æ„é€ å¯åŠ¨å‘½ä»¤ (å»æ‰äº†æŠ¥é”™çš„ --system_prompt)\n",
    "    cmd = (\n",
    "        f\"llamafactory-cli webchat \"\n",
    "        f\"--model_name_or_path {base_model_path} \"\n",
    "        f\"--adapter_name_or_path {adapter_path} \"\n",
    "        f\"--template qwen \"\n",
    "        f\"--finetuning_type lora \"\n",
    "        f\"--quantization_bit 4 \"\n",
    "    )\n",
    "\n",
    "    # è¿è¡Œå‘½ä»¤\n",
    "    get_ipython().system(f\"export GRADIO_SHARE=1 && {cmd}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "VE14kvLihcnk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770818309264,
     "user_tz": -480,
     "elapsed": 7627239,
     "user": {
      "displayName": "minglei F",
      "userId": "11971805550269750385"
     }
    },
    "outputId": "5ec51b6a-67f4-4b31-fb92-bf826847b5ff"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… æ‰¾åˆ°æœ€æ–°æ¨¡å‹æƒé‡: /content/LLaMA-Factory/saves/Qwen2.5-7B-Instruct/lora/train_2026-02-11-09-48-53/checkpoint-100\n",
      "ğŸš€ æ­£åœ¨å¯åŠ¨ Raymond ä¸“å±èŠå¤©å®¤...\n",
      "âš ï¸ æ³¨æ„ï¼šè¿›å…¥ç½‘é¡µåï¼Œè¯·æ‰‹åŠ¨åœ¨å·¦ä¾§ 'System Prompt' æ¡†ä¸­è¾“å…¥ï¼š\n",
      "   You are Raymond, a humorous chatter with a unique sarcastic style.\n",
      "Visit http://ip:port for Web UI, e.g., http://127.0.0.1:7860\n",
      "[INFO|configuration_utils.py:667] 2026-02-11 11:51:36,743 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "[INFO|configuration_utils.py:739] 2026-02-11 11:51:36,746 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": null,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 1000000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:667] 2026-02-11 11:51:38,766 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "[INFO|configuration_utils.py:739] 2026-02-11 11:51:38,767 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": null,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 1000000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:667] 2026-02-11 11:51:38,856 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "[INFO|configuration_utils.py:739] 2026-02-11 11:51:38,857 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": null,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 1000000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:667] 2026-02-11 11:51:40,059 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "[INFO|configuration_utils.py:739] 2026-02-11 11:51:40,060 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": null,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 1000000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[WARNING|logging.py:327] 2026-02-11 11:51:40,060 >> `torch_dtype` is deprecated! Use `dtype` instead!\n",
      "[INFO|2026-02-11 11:51:40] llamafactory.model.model_utils.quantization:144 >> Quantizing model to 4 bit with bitsandbytes.\n",
      "[INFO|2026-02-11 11:51:40] llamafactory.model.model_utils.kv_cache:144 >> KV cache is enabled for faster generation.\n",
      "[INFO|modeling_utils.py:732] 2026-02-11 11:51:43,676 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:801] 2026-02-11 11:51:43,677 >> Will use dtype=torch.bfloat16 as defined in model's config object\n",
      "[INFO|configuration_utils.py:1014] 2026-02-11 11:51:43,678 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"use_cache\": true\n",
      "}\n",
      "\n",
      "Loading weights: 100% 339/339 [00:57<00:00,  5.85it/s, Materializing param=model.norm.weight] \n",
      "[INFO|configuration_utils.py:967] 2026-02-11 11:52:42,205 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/generation_config.json\n",
      "[INFO|configuration_utils.py:1014] 2026-02-11 11:52:42,205 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.05,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "[INFO|2026-02-11 11:52:42] llamafactory.model.model_utils.attention:144 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2026-02-11 11:52:43] llamafactory.model.adapter:144 >> Loaded adapter(s): /content/LLaMA-Factory/saves/Qwen2.5-7B-Instruct/lora/train_2026-02-11-09-48-53/checkpoint-100\n",
      "[INFO|2026-02-11 11:52:43] llamafactory.model.loader:144 >> all params: 7,635,801,600\n",
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* Running on public URL: https://6bcd41e646c830ff39.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/LLaMA-Factory/src/llamafactory/data/formatter.py\", line 152, in apply\n",
      "    tools = json.loads(content)\n",
      "            ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/json/decoder.py\", line 338, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/json/decoder.py\", line 356, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gradio/queueing.py\", line 759, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2191, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1710, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gradio/utils.py\", line 760, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gradio/utils.py\", line 751, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/anyio/to_thread.py\", line 63, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 2502, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 986, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gradio/utils.py\", line 734, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gradio/utils.py\", line 898, in gen_wrapper\n",
      "    response = next(iterator)\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/content/LLaMA-Factory/src/llamafactory/webui/chatter.py\", line 218, in stream\n",
      "    for new_text in self.stream_chat(\n",
      "                    ^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/LLaMA-Factory/src/llamafactory/chat/chat_model.py\", line 135, in stream_chat\n",
      "    yield task.result()\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 456, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/content/LLaMA-Factory/src/llamafactory/chat/chat_model.py\", line 150, in astream_chat\n",
      "    async for new_token in self.engine.stream_chat(\n",
      "  File \"/content/LLaMA-Factory/src/llamafactory/chat/hf_engine.py\", line 394, in stream_chat\n",
      "    stream = self._stream_chat(*input_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/LLaMA-Factory/src/llamafactory/chat/hf_engine.py\", line 281, in _stream_chat\n",
      "    gen_kwargs, _ = HuggingfaceEngine._process_args(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/LLaMA-Factory/src/llamafactory/chat/hf_engine.py\", line 107, in _process_args\n",
      "    prompt_ids, _ = template.encode_oneturn(tokenizer, paired_messages, system, tools)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/LLaMA-Factory/src/llamafactory/data/template.py\", line 67, in encode_oneturn\n",
      "    encoded_messages = self._encode(tokenizer, messages, system, tools)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/LLaMA-Factory/src/llamafactory/data/template.py\", line 150, in _encode\n",
      "    tool_text = self.format_tools.apply(content=tools)[0] if tools else \"\"\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/LLaMA-Factory/src/llamafactory/data/formatter.py\", line 155, in apply\n",
      "    raise RuntimeError(f\"Invalid JSON format in tool description: {str([content])}.\")  # flat string\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Invalid JSON format in tool description: ['ä½ å«ä»€ä¹ˆ'].\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 3043, in block_thread\n",
      "    time.sleep(0.1)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 24, in main\n",
      "    launcher.launch()\n",
      "  File \"/content/LLaMA-Factory/src/llamafactory/launcher.py\", line 162, in launch\n",
      "    run_web_demo()\n",
      "  File \"/content/LLaMA-Factory/src/llamafactory/webui/interface.py\", line 106, in run_web_demo\n",
      "    create_web_demo().queue().launch(share=gradio_share, server_name=server_name, inbrowser=True)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2950, in launch\n",
      "    self.block_thread()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 3047, in block_thread\n",
      "    self.server.close()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gradio/http_server.py\", line 69, in close\n",
      "    self.thread.join(timeout=5)\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1153, in join\n",
      "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1169, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Killing tunnel 0.0.0.0:7860 <> https://6bcd41e646c830ff39.gradio.live\n",
      "^C\n"
     ]
    }
   ]
  }
 ]
}
